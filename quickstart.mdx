---
title: "⭐️ Quickstart"
description: "Setup your analytics stack in 10 minutes"
---

## System Requirements

Python version 3.10 or higher

## What's Vinyl?

Vinyl is Turntable's framework for developing analytics infrastructure.

Vinyl is cross-dialect, evaluated lazily and integrates with your existing data stack. Vinyl is designed to be a data platform that can be accessed in places like BI tools, AI agents or data products and works the same if you have 10,000 rows of data or 100M.

Vinyl constructs SQL queries under the hood to avoid pulling all of your data onto your machine and it supports data sources from cloud storage like S3, databases like Postgres or data warehouses like Bigquery.

## Install Vinyl

First, we have to install the vinyl package

```bash
pip install vinyl-core
```

## Create a Project

Navigate to you're preferred path and create a new project. For this quickstart, we recommended using our provided sample data by passing the `--demo` flag.

This creates a Vinyl project for you with the following structure:

```bash
vinyl init my_shop --demo
```

## Create a Source

Sources are typed data schemas that help Vinyl understand what data is available. Sources can data warehouses (Snowflake, Bigquery), data lakes (Delta), cloud storage (S3, GCS) or even local files. See all of the [supported connections here](/concepts/sources).

Our demo project contains 3 sample csv from the [Ecuador Grocery Store](https://www.kaggle.com/competitions/store-sales-time-series-forecasting) data under the `data/` folder:

- **stores.csv**: A list of 54 grocery stores and their locations
- **store_num_transactions.csv**: The number of transactions at each store over time (2013-2017)
- **holiday_events.csv**: holiday dates in Ecuador (2013-2017)

First, create a Resource. A resource serves as your connection to locations where you want to access source data.

There's a `LocalFilesystem` Resource created for you that helps pull in our local files.

```python my_project/resources.py
from vinyl import resource
from vinyl.connectors import TableFileConnector

@resource
def local_filesystem():
    return TableFileConnector(path="data")
```

We can now generate source files for our upstream data. Use this Vinyl CLI command to generate source files:

```bash
vinyl sources generate
```

The result should be 3 files under `sources/local_filesystem/` that map to each csv file:

- **stores.py**
- **holiday_events.py**
- **store_num_transactions.py**

Sources help validate breaking changes, propagate metadata and power development ergonomics like column-level lineage and column autocomplete. [Federated joins](/concepts/sources) are supported for most connectors (ex: Bigquery joined with Postgres) for data across resources.

Sources can be imported across your vinyl project or used in notebooks for analysis.

Our Stores source looks like this for example:

```python my_store/sources/local_filesystem/stores.py
from vinyl import source, types as t
from my_shop.resources import local_filesystem


@source(resource=local_filesystem)
class Stores:
    _table = "stores"
    _unique_name = "local_filesystem.Stores"
    _path = "data/stores.csv"

    store_nbr: t.Int64(nullable=True)
    city: t.String(nullable=True)
    state: t.String(nullable=True)
    type: t.String(nullable=True)
    cluster: t.Int64(nullable=True)
```

In just a few lines of python and a `@source` annotation, we get a typed schema, column level lineage support, powerful autocomplete and query validation across many different SQL dialects and resource types.

Now let's do something with this source data.

## Create a Model

Models are semantic representations of your data. Models can be used in notebooks for analysis, embedded in BI tools, data products or referenced in other parts of your project. Vinyl provides a [wide range of data transforms](/concepts/models) and complex SQL operations out of the box.

We've created some models for you at `models/models.py`. Let's take a look at some of them.

```python models/models.py
@model(deps=[Stores, StoreNumTransactions])
def top_stores(stores, txns):
    table = stores.join(txns, ["store_nbr"])
    table.aggregate(
        cols={
            "num_transactions": table.transactions.sum()
        },
        by=[
            table.store_nbr,
            table.city,
            table.state
        ],
    )
    return table.sort(
        by=[table.num_transactions.desc()]
    )
```

Models help you build data pipelines in a declarative way. Upstream data is passed in as plain python functions or classes as dependencies in the `@model` annotation. In this example, we pass in the `Stores` source and `StoreNumTransactions` sources from earlier for us to aggregate to find the stores with the most transactions.

This kind of data work usually involves a lot of brittle SQL and data wrangling in python is often a chore. What makes Vinyl different here is our [pipelined approach](/introduction) to data modeling, a [simple API](/reference) for performing transform operations, and cross-dialect SQL support. You can take the same model code above, change the source from a local file to Snowflake with the same schema and you'll get the same result.

Now, let's preview what this looks like using Vinyl's built-in data preview tool.

```bash
vinyl models preview top_stores
```

![preview](/images/preview.png)

Vinyl's data preview tool is a powerful way to see your data in action. It's a great way to validate your data and see what your models are doing under the hood.

Let's take a look at one more model in our project.

```python models/models.py
@model(deps=[StoreNumTransactions, Stores])
def store_txns(txns, stores):
    table = txns.join(
        stores,
        [
            stores.store_nbr == txns.store_nbr,
        ],
    )
    return table
```

This model doesn't have any aggregation but it does join the `StoreNumTransactions` and `Stores` sources together. This model has the number of transactions per calendar date along with some store information.

If we use `vinyl preview --model store_txns`, the resulting data looks like:

![preview_metric](/images/metric_preview.png)

Time series data is notoriously hard to model. You have to think about time buckets, aggregation windows, missing dates and dimensions. Oftentimes your first guess (weekly sales in the last 30 days) ends up being wrong when someone wants sales data quarterly.

Luckily, Vinyl has a powerful abstraction for this called Metrics.

## Create a Metric

Metrics are a powerful tool in Vinyl that allows for tracking time series data. Metrics auto generate complicated timeseries SQL and can be queried across dimensions and time buckets dynamically.

Create a metric by passing in a `MetricStore`. We'll reference the same `store_txns` model from above.

```python
@model(deps=[store_txns, MetricStore], publish=True)
def average_fare_metric(s, store):
    store.metric(
        tbl=s,
        by=.store_and_fwd_flag,
        ts=b.pickup_at,
        cols={
            "total_amount": b.total_amount.mean()
        },
    )
    return store
```

Let's preview that our metric has data using the CLI command:

```bash
vinyl model preview average_fare_metric --30d
```

## Deployment

While vinyl is a great data modeling and analysis tool on it's own, it's also designed to be a data platform that can be accessed in places like BI tools, AI agents or data products.

You can orchestrate and deploy your vinyl project using the CLI. By default, vinyl can run workloads and serve data locally. For production, we recommend [self-hosting](/concepts/deployment) or using [Turntable cloud](/concepts/deployment).

```bash
vinyl project deploy
```

Vinyl uses fast OLAP caching to support sub second queries on 10s of millions of rows of data. Try serving models and metrics using the CLI command:

```bash
vinyl project serve -p 5432
```

Once running, you can serve models and metrics via a postgres connector or HTTP API call. You can query your deployed Vinyl project in numerous BI and data tools (latestearn more about [integrations](/concepts/integrations) here).

Try using a local postgres client (like psql) to query your deployed models and metrics.

```
psql -h 0.0.0.0 -p 5432 -c "select * from average_fare_metric where bucket = 'monthly' limit 10";
```

Or via a HTTP API call:

```bash
curl -X POST -H "Content-Type: application/json" localhost:3000/model/average_fare_metric -d '{"time_bucket": "monthly", "limit": 10}'
```
